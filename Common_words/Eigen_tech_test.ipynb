{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize,sent_tokenize,FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import glob\n",
    "import operator\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#utility function to remove stopwords and other simbols\n",
    "def removeStopwords(s):\n",
    "    stopwords_list = stopwords.words('english') + list(string.punctuation)\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    return [w for w in s if w not in stopwords_list and len(regex.sub('', w))>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utility function to create the word dictionary of frequent words\n",
    "def createWordDict(list_of_tokenized_sentences):\n",
    "    all_words = [item for sublist in list_of_tokenized_sentences for item in sublist]\n",
    "    freq_dist = FreqDist(all_words)\n",
    "    sorted_x = sorted(freq_dist.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    toReturn = {}\n",
    "    for pair in sorted_x:\n",
    "        if pair[1]>1:\n",
    "            toReturn[pair[0]]={}\n",
    "    return toReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utility function to create a list of tokens without stopwords for a given sentence\n",
    "def createCleanSentence(sentence):\n",
    "    word_list = word_tokenize(sentence.lower())\n",
    "    clean_word_list = removeStopwords(word_list)\n",
    "    return clean_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main function to create a dictionary with the requested results\n",
    "# input : the documents path, see example below\n",
    "def createHashtags(document_path):\n",
    "    files = glob.glob(document_path)\n",
    "    documents = []\n",
    "    for f in files:\n",
    "        with open(f, 'r') as input_f:\n",
    "            documents.append(input_f.read())\n",
    "    all_documents = '\\n'.join(documents)\n",
    "    sentence_list = sent_tokenize(all_documents.decode('utf-8'))\n",
    "    clean_sentence_list = []\n",
    "    for s in sentence_list:\n",
    "        clean_sentence_list.append(createCleanSentence(s))\n",
    "    word_dict = createWordDict(clean_sentence_list)\n",
    "    frequent_words = word_dict.keys()\n",
    "    for doc,doc_name in zip(documents,files):\n",
    "        doc_sentence_list = sent_tokenize(doc.decode('utf-8'))\n",
    "        doc_clean_sentence_list = []\n",
    "        for s in doc_sentence_list:\n",
    "            doc_clean_sentence_list.append(createCleanSentence(s))\n",
    "        for clean_s,sen in zip(doc_clean_sentence_list,doc_sentence_list):\n",
    "            for word in clean_s:\n",
    "                if word in frequent_words:\n",
    "                    if doc in word_dict[word]:\n",
    "                        word_dict[word][doc_name].append(sen)\n",
    "                    else:\n",
    "                        word_dict[word][doc_name]=[sen]\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utility function to save the results into a csv file with elements separated by delimiter: \";--;\"\n",
    "def saveHashtags(hashtag_dict):\n",
    "    delim = ';--;'\n",
    "    header = 'Word;--;Document;--;Sentence'\n",
    "    with open('output.csv','w') as out:\n",
    "        out.write(header+'\\n')\n",
    "        for word,mapping in hashtag_dict.iteritems():\n",
    "            for doc,sents in mapping.iteritems():\n",
    "                for s in sents:\n",
    "                    toWrite = word+delim+doc+delim+s.replace('\\n','')+'\\n'\n",
    "                    out.write(toWrite.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = createHashtags('*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saveHashtags(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets read the created output and put it into a dataframe object\n",
    "read_output = pd.read_csv('output.csv',delimiter=';--;',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Document</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neighbors</td>\n",
       "      <td>doc3.txt</td>\n",
       "      <td>We know that we've been called in churches and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neighbors</td>\n",
       "      <td>doc2.txt</td>\n",
       "      <td>I've seen it in the workers who would rather c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>teach</td>\n",
       "      <td>doc3.txt</td>\n",
       "      <td>You teach law school, you're a civil rights at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>teach</td>\n",
       "      <td>doc5.txt</td>\n",
       "      <td>We can teach the soldiers to fight and police ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ended</td>\n",
       "      <td>doc4.txt</td>\n",
       "      <td>His ideas about how Kenya should progress ofte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Document                                           Sentence\n",
       "0  neighbors  doc3.txt  We know that we've been called in churches and...\n",
       "1  neighbors  doc2.txt  I've seen it in the workers who would rather c...\n",
       "2      teach  doc3.txt  You teach law school, you're a civil rights at...\n",
       "3      teach  doc5.txt  We can teach the soldiers to fight and police ...\n",
       "4      ended  doc4.txt  His ideas about how Kenya should progress ofte..."
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can look at the results in a readable format, and we can group the result if we want\n",
    "read_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ability\n",
      "         Word  Document                                           Sentence\n",
      "2050  ability  doc4.txt  Of course, in the end, one of the strongest we...\n",
      "2051  ability  doc6.txt  I also saw their ability to ensure that shortc...\n",
      "2052  ability  doc5.txt  But we need to also move towards more conditio...\n"
     ]
    }
   ],
   "source": [
    "# Lets group the results by word and show the first result\n",
    "grouped = read_output.groupby('Word')\n",
    "for name, g in grouped:\n",
    "    print name\n",
    "    print g.head()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
