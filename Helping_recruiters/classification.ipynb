{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from matplotlib.pyplot import plot\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (20, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utility function to remove html tag from text\n",
    "def cleanhtml(raw_html):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  cleantext = re.sub(cleanr, ' ', raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#utility function to transform text into vector representation\n",
    "def text2vector(text,count_vect):\n",
    "    X_train_counts = count_vect.transform(text)\n",
    "    tf_transformer = TfidfTransformer(use_idf=False)\n",
    "    X_train_tf = tf_transformer.fit_transform(X_train_counts)\n",
    "#     print X_train_tf.shape\n",
    "    return X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function to train a classifier with cross validation technique\n",
    "def trainWithCrossValidation(clf,data,labels,test_size,n_splits):\n",
    "    ss = ShuffleSplit(n_splits=n_splits, test_size=test_size,random_state=42)\n",
    "    for train, test in ss.split(data):\n",
    "        X_train = [data[i] for i in train]\n",
    "        X_test = [data[i] for i in test]\n",
    "        y_train = [labels[i] for i in train]\n",
    "        y_test = [labels[i] for i in test]\n",
    "        X_train_titles = [x[0] for x in X_train]\n",
    "        X_train_description = [x[1] for x in X_train]\n",
    "        X_test_titles = [x[0] for x in X_test]\n",
    "        X_test_description = [x[1] for x in X_test]\n",
    "        count_vect = CountVectorizer()\n",
    "        count_vect.fit(X_train_description)\n",
    "        X_train_titles_vec = text2vector(X_train_titles,count_vect)\n",
    "        X_train_description_vec = text2vector(X_train_description,count_vect)\n",
    "        X_test_titles_vec = text2vector(X_test_titles,count_vect)\n",
    "        X_test_description_vec = text2vector(X_test_description,count_vect)\n",
    "        x_train_vec = sparse.hstack([X_train_titles_vec,X_train_description_vec])\n",
    "        x_test_vec = sparse.hstack([X_test_titles_vec,X_test_description_vec])\n",
    "        clf.fit(x_train_vec, y_train)\n",
    "        prediction = clf.predict(x_test_vec)\n",
    "        print accuracy_score(y_test,prediction)\n",
    "        confusion_matrix(y_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "dataset = pd.read_csv('reed_data_scientist_task_jobs_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "      <th>job_sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19549447</td>\n",
       "      <td>Geography Teacher</td>\n",
       "      <td>&lt;p&gt;Forde Education are looking to recruit a Te...</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7447537</td>\n",
       "      <td>PPA Cover teacher</td>\n",
       "      <td>Teachers Plus is seeking to employ a fully qua...</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26969327</td>\n",
       "      <td>Higher Level Teaching Assistant</td>\n",
       "      <td>We are currently recruiting High Level Teachin...</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7447589</td>\n",
       "      <td>Yr 2 Teacher</td>\n",
       "      <td>A suitably qualified and experienced Yr 2 Teac...</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26978624</td>\n",
       "      <td>Science Teachers</td>\n",
       "      <td>&lt;strong&gt;Job Description&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt;Mo...</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     job_id                        job_title  \\\n",
       "0  19549447                Geography Teacher   \n",
       "1   7447537                PPA Cover teacher   \n",
       "2  26969327  Higher Level Teaching Assistant   \n",
       "3   7447589                     Yr 2 Teacher   \n",
       "4  26978624                 Science Teachers   \n",
       "\n",
       "                                     job_description job_sector  \n",
       "0  <p>Forde Education are looking to recruit a Te...  Education  \n",
       "1  Teachers Plus is seeking to employ a fully qua...  Education  \n",
       "2  We are currently recruiting High Level Teachin...  Education  \n",
       "3  A suitably qualified and experienced Yr 2 Teac...  Education  \n",
       "4  <strong>Job Description</strong><br /><br />Mo...  Education  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets see how the dataset looks like\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check how big is the dataset and if there are some nan values or duplicates\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dropna().drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IT & Telecoms            1000\n",
       "Transport & Logistics    1000\n",
       "Engineering              1000\n",
       "Legal                    1000\n",
       "Sales                    1000\n",
       "Education                1000\n",
       "Name: job_sector, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check if the dataset is balanced, luckly it is! \n",
    "dataset.job_sector.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considerations\n",
    "\n",
    "With problems based on text data, there is a temptation to use word2vec as it usually works so well!\n",
    "\n",
    "But one thing to keep in mind with word2vec and similar solutions is that they need alot of data to shine.\n",
    "\n",
    "In this problem there is not enough training data to properly train a specialized word2vec model.\n",
    "\n",
    "So I am going to use the standard bag of words approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets make a copy of the original dataset and trasform the job_sector labels into numerical values\n",
    "data = dataset.copy()\n",
    "mymap = {'Education':0, 'Engineering': 1, 'Legal': 2, 'IT & Telecoms':3, 'Sales':4, 'Transport & Logistics':5 }\n",
    "data['labels'] = data.job_sector.apply(lambda s: mymap.get(s) if s in mymap else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1000\n",
       "2    1000\n",
       "5    1000\n",
       "1    1000\n",
       "4    1000\n",
       "0    1000\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets remove all the html tags from the job_description\n",
    "data.job_description = data.job_description.apply(lambda x: cleanhtml(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "      <th>job_sector</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19549447</td>\n",
       "      <td>Geography Teacher</td>\n",
       "      <td>Forde Education are looking to recruit a Teac...</td>\n",
       "      <td>Education</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7447537</td>\n",
       "      <td>PPA Cover teacher</td>\n",
       "      <td>Teachers Plus is seeking to employ a fully qua...</td>\n",
       "      <td>Education</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26969327</td>\n",
       "      <td>Higher Level Teaching Assistant</td>\n",
       "      <td>We are currently recruiting High Level Teachin...</td>\n",
       "      <td>Education</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7447589</td>\n",
       "      <td>Yr 2 Teacher</td>\n",
       "      <td>A suitably qualified and experienced Yr 2 Teac...</td>\n",
       "      <td>Education</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26978624</td>\n",
       "      <td>Science Teachers</td>\n",
       "      <td>Job Description   Most Secondary Schools requ...</td>\n",
       "      <td>Education</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     job_id                        job_title  \\\n",
       "0  19549447                Geography Teacher   \n",
       "1   7447537                PPA Cover teacher   \n",
       "2  26969327  Higher Level Teaching Assistant   \n",
       "3   7447589                     Yr 2 Teacher   \n",
       "4  26978624                 Science Teachers   \n",
       "\n",
       "                                     job_description job_sector  labels  \n",
       "0   Forde Education are looking to recruit a Teac...  Education       0  \n",
       "1  Teachers Plus is seeking to employ a fully qua...  Education       0  \n",
       "2  We are currently recruiting High Level Teachin...  Education       0  \n",
       "3  A suitably qualified and experienced Yr 2 Teac...  Education       0  \n",
       "4   Job Description   Most Secondary Schools requ...  Education       0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets select just the data that we need to work with\n",
    "data_for_classifier = data[['job_title','job_description']]\n",
    "labels_for_classifier = data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3897 5628 1756 ..., 5226 5390  860] [1782 3917  221 ..., 2846 5799 1765]\n",
      "[1270 2385  708 ..., 1514 2313 1451] [4796 2596 3529 ..., 2666 3361 4508]\n",
      "[4838   96 4701 ..., 3270 3884 3279] [2752 2027  224 ..., 2915 2979 4408]\n",
      "[ 465 1259 3954 ...,  517  978 2739] [4570 5667 5425 ..., 3072 2614 4064]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "def trainWithCrossValidation(clf,data,labels,test_size,n_splits):\n",
    "    ss = ShuffleSplit(n_splits=n_splits, test_size=test_size,random_state=42)\n",
    "    for train, test in ss.split(data):\n",
    "        X_train = [data[i] for i in train]\n",
    "        X_test = [data[i] for i in test]\n",
    "        y_train = [labels[i] for i in train]\n",
    "        y_test = [labels[i] for i in test]\n",
    "        X_train_titles = [x[0] for x in X_train]\n",
    "        X_train_description = [x[1] for x in X_train]\n",
    "        X_test_titles = [x[0] for x in X_test]\n",
    "        X_test_description = [x[1] for x in X_test]\n",
    "        count_vect = CountVectorizer()\n",
    "        count_vect.fit(X_train_description)\n",
    "        X_train_titles_vec = text2vector(X_train_titles,count_vect)\n",
    "        X_train_description_vec = text2vector(X_train_description,count_vect)\n",
    "        X_test_titles_vec = text2vector(X_test_titles,count_vect)\n",
    "        X_test_description_vec = text2vector(X_test_description,count_vect)\n",
    "        x_train_vec = sparse.hstack([X_train_titles_vec,X_train_description_vec])\n",
    "        x_test_vec = sparse.hstack([X_test_titles_vec,X_test_description_vec])\n",
    "        clf.fit(x_train_vec, y_train)\n",
    "        prediction = clf.predict(x_test_vec)\n",
    "        print accuracy_score(y_test,prediction)\n",
    "        confusion_matrix(y_test,prediction)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets split the data into train and test set, and remember the answer is 42!\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_for_classifier.as_matrix(),labels_for_classifier.tolist(),test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now I need to separate the title from the description so later I can join the vector representations of them\n",
    "X_train_titles = [x[0] for x in X_train]\n",
    "X_train_description = [x[1] for x in X_train]\n",
    "X_test_titles = [x[0] for x in X_test]\n",
    "X_test_description = [x[1] for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 824, 4: 811, 5: 806, 2: 796, 0: 785, 1: 778})\n",
      "Counter({1: 222, 0: 215, 2: 204, 5: 194, 4: 189, 3: 176})\n"
     ]
    }
   ],
   "source": [
    "#lets check if the distribution of the classes between train and test is still balanced\n",
    "print Counter(y_train)\n",
    "print Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 19311)\n",
      "(4800, 19311)\n",
      "(1200, 19311)\n",
      "(1200, 19311)\n"
     ]
    }
   ],
   "source": [
    "#Lets fit the CountVectorizer with the training data and use it to transform training and testing data\n",
    "count_vect = CountVectorizer()\n",
    "count_vect.fit(X_train_description)\n",
    "X_train_titles_vec = text2vector(X_train_titles,count_vect)\n",
    "X_train_description_vec = text2vector(X_train_description,count_vect)\n",
    "X_test_titles_vec = text2vector(X_test_titles,count_vect)\n",
    "X_test_description_vec = text2vector(X_test_description,count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 38622)\n",
      "(1200, 38622)\n"
     ]
    }
   ],
   "source": [
    "#Lets join title vectors and description vectors into one bigger and stronger vector!\n",
    "x_train_vec = sparse.hstack([X_train_titles_vec,X_train_description_vec])\n",
    "x_test_vec = sparse.hstack([X_test_titles_vec,X_test_description_vec])\n",
    "print x_train_vec.shape\n",
    "print x_test_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Now lets try to train different models with standard parameters and see which ones performs best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[199,   5,   0,   3,   2,   6],\n",
       "       [  0, 194,   0,  14,   7,   7],\n",
       "       [  1,   2, 196,   2,   1,   2],\n",
       "       [  0,   3,   0, 163,   8,   2],\n",
       "       [  0,   4,   0,   1, 182,   2],\n",
       "       [  0,   2,   0,   6,   8, 178]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM classifier with linear kernel\n",
    "clf1 = SVC(kernel='linear').fit(x_train_vec, y_train)\n",
    "prediction1 = clf1.predict(x_test_vec)\n",
    "print accuracy_score(y_test,prediction1)\n",
    "confusion_matrix(y_test,prediction1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926666666667\n",
      "0.931666666667\n",
      "0.928333333333\n",
      "0.925833333333\n"
     ]
    }
   ],
   "source": [
    "#SVM classifier with cross validation\n",
    "clf1 = SVC(kernel='linear')\n",
    "trainWithCrossValidation(clf1,data_for_classifier.as_matrix(),labels_for_classifier.tolist(),0.2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[199,   2,   1,   3,   5,   5],\n",
       "       [  0, 183,   0,  21,  10,   8],\n",
       "       [  0,   1, 193,   5,   4,   1],\n",
       "       [  0,   2,   0, 163,  10,   1],\n",
       "       [  0,   3,   0,   1, 184,   1],\n",
       "       [  1,   6,   1,   4,  12, 170]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Multinomial Naive Bayes classifier\n",
    "clf2 = MultinomialNB().fit(x_train_vec, y_train)\n",
    "prediction2 = clf2.predict(x_test_vec)\n",
    "print accuracy_score(y_test,prediction2)\n",
    "confusion_matrix(y_test,prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n",
      "0.92\n",
      "0.9275\n",
      "0.91\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes classifier with cross validation\n",
    "clf2 = MultinomialNB()\n",
    "trainWithCrossValidation(clf2,data_for_classifier.as_matrix(),labels_for_classifier.tolist(),0.2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.920833333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[201,   1,   2,   4,   2,   5],\n",
       "       [  1, 193,   1,  11,   8,   8],\n",
       "       [  0,   1, 197,   2,   3,   1],\n",
       "       [  0,  10,   0, 154,   9,   3],\n",
       "       [  0,   2,   0,   3, 183,   1],\n",
       "       [  0,   4,   3,   3,   7, 177]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stochastic Gradient Descent classifier\n",
    "clf3 = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42).fit(x_train_vec, y_train)\n",
    "prediction3 = clf3.predict(x_test_vec)\n",
    "print accuracy_score(y_test,prediction3)\n",
    "confusion_matrix(y_test,prediction3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.920833333333\n",
      "0.93\n",
      "0.9275\n",
      "0.92\n"
     ]
    }
   ],
   "source": [
    "#Stochastic Gradient Descent classifier with cross validation\n",
    "clf3 = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "trainWithCrossValidation(clf3,data_for_classifier.as_matrix(),labels_for_classifier.tolist(),0.2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.938333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[205,   2,   1,   3,   4,   0],\n",
       "       [  0, 200,   0,  10,   5,   7],\n",
       "       [  0,   1, 199,   2,   0,   2],\n",
       "       [  0,   3,   0, 165,   6,   2],\n",
       "       [  0,   5,   0,   4, 179,   1],\n",
       "       [  0,   5,   1,   4,   6, 178]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Neural network: the Multilayer perceptron\n",
    "clf4 = MLPClassifier( random_state=42).fit(x_train_vec, y_train)\n",
    "prediction4 = clf4.predict(x_test_vec)\n",
    "print accuracy_score(y_test,prediction4)\n",
    "confusion_matrix(y_test,prediction4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.938333333333\n",
      "0.946666666667\n",
      "0.945\n",
      "0.938333333333\n"
     ]
    }
   ],
   "source": [
    "#Neural network: the Multilayer perceptron with cross validation\n",
    "clf4 = MLPClassifier( random_state=42)\n",
    "trainWithCrossValidation(clf4,data_for_classifier.as_matrix(),labels_for_classifier.tolist(),0.2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[202,   1,   0,   7,   1,   4],\n",
       "       [  0, 193,   1,   9,   9,  10],\n",
       "       [  0,   0, 199,   2,   1,   2],\n",
       "       [  0,   6,   0, 161,   7,   2],\n",
       "       [  0,   4,   1,   1, 182,   1],\n",
       "       [  0,   6,   0,   3,  10, 175]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random forest classifier\n",
    "clf5 = RandomForestClassifier(n_estimators=200).fit(x_train_vec,y_train)\n",
    "prediction5 = clf5.predict(x_test_vec)\n",
    "print accuracy_score(y_test,prediction5)\n",
    "confusion_matrix(y_test,prediction5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.919166666667\n",
      "0.939166666667\n",
      "0.938333333333\n",
      "0.926666666667\n"
     ]
    }
   ],
   "source": [
    "#Random forest classifier with cross validation\n",
    "clf5 = RandomForestClassifier(n_estimators=200)\n",
    "trainWithCrossValidation(clf5,data_for_classifier.as_matrix(),labels_for_classifier.tolist(),0.2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.870833333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[193,   4,   1,   5,   8,   4],\n",
       "       [  3, 190,   1,  10,   8,  10],\n",
       "       [  3,   4, 185,   4,   5,   3],\n",
       "       [  3,  27,   0, 131,  11,   4],\n",
       "       [  3,   3,   0,   2, 181,   0],\n",
       "       [  5,   9,   1,   5,   9, 165]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KNN classifier\n",
    "clf6 = KNeighborsClassifier(n_neighbors=15).fit(x_train_vec,y_train)\n",
    "prediction6 = clf6.predict(x_test_vec)\n",
    "print accuracy_score(y_test,prediction6)\n",
    "confusion_matrix(y_test,prediction6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.870833333333\n",
      "0.865833333333\n",
      "0.876666666667\n",
      "0.848333333333\n"
     ]
    }
   ],
   "source": [
    "#KNN classifier with cross validation\n",
    "clf6 = KNeighborsClassifier(n_neighbors=15)\n",
    "trainWithCrossValidation(clf6,data_for_classifier.as_matrix(),labels_for_classifier.tolist(),0.2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considerations\n",
    "\n",
    "All of the classifers perform quite well, but we want a better solution.\n",
    "\n",
    "Lets select the best two models and try to optimize them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n",
      "RandomizedSearchCV took 237.22 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.926 (std: 0.002)\n",
      "Parameters: {'kernel': 'sigmoid', 'C': 98, 'gamma': 0.008}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.924 (std: 0.002)\n",
      "Parameters: {'kernel': 'rbf', 'C': 70, 'gamma': 0.0034}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.923 (std: 0.001)\n",
      "Parameters: {'kernel': 'rbf', 'C': 59, 'gamma': 0.0034}\n",
      "\n",
      "MLPC\n",
      "RandomizedSearchCV took 8403.01 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.942 (std: 0.007)\n",
      "Parameters: {'solver': 'adam', 'activation': 'tanh', 'max_iter': 190, 'batch_size': 260, 'learning_rate_init': 0.0001, 'momentum': 0.4, 'hidden_layer_sizes': 40}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.940 (std: 0.006)\n",
      "Parameters: {'solver': 'adam', 'activation': 'tanh', 'max_iter': 30, 'batch_size': 110, 'learning_rate_init': 0.0001, 'momentum': 0.9, 'hidden_layer_sizes': 170}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.937 (std: 0.005)\n",
      "Parameters: {'solver': 'adam', 'activation': 'identity', 'max_iter': 130, 'batch_size': 110, 'learning_rate_init': 0.01, 'momentum': 0.0, 'hidden_layer_sizes': 60}\n",
      "\n",
      "RandomForest\n",
      "RandomizedSearchCV took 159.82 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.934 (std: 0.009)\n",
      "Parameters: {'n_estimators': 360, 'n_jobs': -1, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.933 (std: 0.009)\n",
      "Parameters: {'n_estimators': 430, 'n_jobs': -1, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.933 (std: 0.009)\n",
      "Parameters: {'n_estimators': 310, 'n_jobs': -1, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.933 (std: 0.007)\n",
      "Parameters: {'n_estimators': 170, 'n_jobs': -1, 'criterion': 'gini'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c1 = SVC()\n",
    "k=['rbf', 'linear','poly','sigmoid']\n",
    "c= range(1,100)\n",
    "g=np.arange(1e-4,1e-2,0.0001)\n",
    "g=g.tolist()\n",
    "param_dist1=dict(kernel=k, C=c, gamma=g)\n",
    "\n",
    "\n",
    "c2 = MLPClassifier( random_state=42)\n",
    "batch_size = range(10,300,10)\n",
    "max_iter = range(10,300,10)\n",
    "learning_rate_init = [0.0001,0.001, 0.01, 0.1,0.2]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "activation = ['identity', 'logistic', 'tanh', 'relu']\n",
    "solver = ['adam','sgd','lbfgs']\n",
    "hidden_layer_sizes = range(10,300,10)\n",
    "param_dist2 = dict(batch_size=batch_size, max_iter=max_iter,learning_rate_init=learning_rate_init,momentum=momentum,activation=activation,solver=solver,hidden_layer_sizes=hidden_layer_sizes)\n",
    "\n",
    "\n",
    "c3 = RandomForestClassifier()\n",
    "n_estimators=range(10,500,10)\n",
    "criterion = ['gini','entropy']\n",
    "n_jobs = [-1]\n",
    "param_dist3 = dict(n_estimators=n_estimators,criterion=criterion,n_jobs=n_jobs)\n",
    "\n",
    "\n",
    "classifiers = []\n",
    "classifiers.append(['SVC',c1,param_dist1])\n",
    "classifiers.append(['MLPC',c2,param_dist2])\n",
    "classifiers.append(['RandomForest',c3,param_dist3])\n",
    "\n",
    "\n",
    "\n",
    "for name,clf,params in classifiers:\n",
    "    print name\n",
    "    n_iter_search = 20\n",
    "    random_search = RandomizedSearchCV(clf, param_distributions=params,\n",
    "                                       n_iter=n_iter_search,n_jobs=-1)\n",
    "\n",
    "    start = time.time()\n",
    "    random_search.fit(x_train_vec, y_train)\n",
    "    print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "          \" parameter settings.\" % ((time.time() - start), n_iter_search))\n",
    "    report(random_search.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of the randomizedSearchCV\n",
    "## SVC\n",
    "RandomizedSearchCV took 237.22 seconds for 20 candidates parameter settings.\n",
    "\n",
    "Model with rank: 1\n",
    "Mean validation score: 0.926 (std: 0.002)\n",
    "Parameters: {'kernel': 'sigmoid', 'C': 98, 'gamma': 0.008}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: 0.924 (std: 0.002)\n",
    "Parameters: {'kernel': 'rbf', 'C': 70, 'gamma': 0.0034}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: 0.923 (std: 0.001)\n",
    "Parameters: {'kernel': 'rbf', 'C': 59, 'gamma': 0.0034}\n",
    "\n",
    "## MLPC\n",
    "RandomizedSearchCV took 8403.01 seconds for 20 candidates parameter settings.\n",
    "\n",
    "Model with rank: 1\n",
    "Mean validation score: 0.942 (std: 0.007)\n",
    "Parameters: {'solver': 'adam', 'activation': 'tanh', 'max_iter': 190, 'batch_size': 260, 'learning_rate_init': 0.0001, 'momentum': 0.4, 'hidden_layer_sizes': 40}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: 0.940 (std: 0.006)\n",
    "Parameters: {'solver': 'adam', 'activation': 'tanh', 'max_iter': 30, 'batch_size': 110, 'learning_rate_init': 0.0001, 'momentum': 0.9, 'hidden_layer_sizes': 170}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: 0.937 (std: 0.005)\n",
    "Parameters: {'solver': 'adam', 'activation': 'identity', 'max_iter': 130, 'batch_size': 110, 'learning_rate_init': 0.01, 'momentum': 0.0, 'hidden_layer_sizes': 60}\n",
    "\n",
    "## RandomForest\n",
    "RandomizedSearchCV took 159.82 seconds for 20 candidates parameter settings.\n",
    "\n",
    "Model with rank: 1\n",
    "Mean validation score: 0.934 (std: 0.009)\n",
    "Parameters: {'n_estimators': 360, 'n_jobs': -1, 'criterion': 'gini'}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: 0.933 (std: 0.009)\n",
    "Parameters: {'n_estimators': 430, 'n_jobs': -1, 'criterion': 'gini'}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: 0.933 (std: 0.009)\n",
    "Parameters: {'n_estimators': 310, 'n_jobs': -1, 'criterion': 'gini'}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: 0.933 (std: 0.007)\n",
    "Parameters: {'n_estimators': 170, 'n_jobs': -1, 'criterion': 'gini'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "As showed in the results, the multilayer perceptron achieved the best results on this type of problem.\n",
    "\n",
    "Given more computational power and time, it would be probably possible to tune it even more, and try different and more complex architectures.\n",
    "\n",
    "Given a bigger dataset it would be intereseting to use word2vec to preprocess the text data. Every industry have its own technical words and a solution like word2vec would be able to associate those specific words to the industry sector, creating a 'meaning' space for each industry.\n",
    "\n",
    "As for the practical use of this results, I think it would be a good solution to show to the client a drop down list with the top 3 most probable industries given by the classifier, or a selection box where the user have to just click on the appropriate industry sector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
